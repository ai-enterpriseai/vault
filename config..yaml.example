base_eval:
  id: rag-eval.v0.1
  name: base_eval
  setup:
    experiment:
      tracking: true
      save_path: "experiments/"
      metadata_fields: ["timestamp", "dataset", "model_versions"]
        
    metrics:
      factuality:
        version: "v0.1"
        name: "factuality" # corresponds to metrics/name.md
        description: "Evaluates factual accuracy of responses"

      relevance:
        version: "v0.1"
        name: "relevance"
        description: "Evaluates response relevance to queries"

  pipeline: 
    processor:
      chunk_size: 512
      chunk_overlap: 128
      min_chunk_size: 128

    embedder:
      dense_model_name: "all-MiniLM-L6-v2" # in sentence transformers
      dense_model_dimension: 384 # must correspond with indexer

    indexer: # qdrant client 
      use_local_db: false
      url: "https://c73ec2ca-fdf3-445c-92ea-febe55cebf46.us-east4-0.gcp.cloud.qdrant.io"
      qdrant_api_key: "YOUR_KEY"
      collection_name: "inhousegpt-participantXX"
      # db_path: "database"
      dense_dim: 384 # must correspond with embedder 

    retriever:
      top_k: 25
      rerank_top_k: 5
      
    manager: 
      templates_dir: "./prompts"
      version: "v0.1"
      prompt: "system_workshop_assistant_de" # corresponds to prompts/name.md
      description: "German language workshop assistant for AI implementation guidance"

    generator:
      primary_model: 
        name: "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo"
        max_tokens: 4096
        temperature: 0.2
        timeout: 30.0
      fallback_model: 
        name: "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo"
        max_tokens: 4096
        temperature: 0.2
        timeout: 30.0
      together_api_key: "YOUR_KEY"
      anthropic_api_key: "YOUR_KEY"
      temperature: 0.7
    
  disclaimer: |
    Evaluation relies on expert-reviewed ground truth data.
    Metrics are based on LLM evaluation which may have inherent biases.
    Results should be reviewed alongside human evaluation.
